{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "#import requests\n",
    "import time \n",
    "import datetime\n",
    "from google.cloud import bigquery\n",
    "import os\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "% pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Authenticate\n",
    "\n",
    "# os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/users/josephpope/Github/Fletcher/reddit-hate-speech-dc35623b7b98.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiates a client\n",
    "\n",
    "# client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean row data from BigQuery \n",
    "\n",
    "# def row_to_dict(row):\n",
    "#     return {k: row[v] for k, v in row._xxx_field_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define relevant IDs in order to pull data from BigQuery account\n",
    "\n",
    "# project_id = 'reddit-hate-speech'\n",
    "# dataset_id = 'reddit_db'\n",
    "# table_id = 'results_20180820_134202'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_ref = client.dataset(dataset_id, project=project_id)\n",
    "# table_ref = dataset_ref.table(table_id)\n",
    "# table = client.get_table(table_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all rows from a table\n",
    "\n",
    "# rows = client.list_rows(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame([row_to_dict(r) for r in rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('reddit_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115420, 17)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop known bots from analysis \n",
    "\n",
    "bots = df[df['author'].str.contains('Bot')]['author'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['author'].isin(bots)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep a copy of the dataframe with minimal text pre-processing in case interesting signals \n",
    "# exist in punctuation or stop words\n",
    "\n",
    "# df1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1.to_csv('reddit_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.style as style\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = df.subreddit.value_counts()[:10].sort_values(ascending=True).plot(kind='barh')\n",
    "\n",
    "ax.set_title('Most Common Subreddits by Comment Count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import ScalarFormatter, FormatStrFormatter\n",
    "popular = df.groupby('subreddit')['subreddit_subscribers'].max().sort_values(ascending=False)\n",
    "\n",
    "ax = popular[:10].sort_values(ascending=True).plot(kind='barh')\n",
    "\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%d'))\n",
    "ax.set_title('Most Subscribed Subreddits');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_text(s):\n",
    "    text = df[df.subreddit == str(s)].body.head()\n",
    "    for i, post in enumerate(text):\n",
    "        print(\"Post: \" + str(i+1) + \" \" + str(post) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_text('worldnews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is this just \"The Jew Show\" now, or is this ju...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wiehe points out that masturbation, hitherto a...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wiehe points out that masturbation, hitherto a...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You think the forcible removal of people is un...</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&gt;Gaza will become its own thing \\n\\nGaza is pr...</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  word_count\n",
       "0  Is this just \"The Jew Show\" now, or is this ju...          17\n",
       "1  Wiehe points out that masturbation, hitherto a...          24\n",
       "2  Wiehe points out that masturbation, hitherto a...          24\n",
       "3  You think the forcible removal of people is un...          51\n",
       "4  >Gaza will become its own thing \\n\\nGaza is pr...         178"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count words\n",
    "\n",
    "df['word_count'] = df['body'].apply(lambda x: len(str(x).split(\" \")))\n",
    "df[['body','word_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count occurrences of 'jewish cowbell' triple parenthesis '(((like this)))'\n",
    "\n",
    "df['cowbell'] = df['body'].apply(lambda x: len([x for x in x.split() if '(((' in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is this just \"The Jew Show\" now, or is this ju...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wiehe points out that masturbation, hitherto a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wiehe points out that masturbation, hitherto a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You think the forcible removal of people is un...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&gt;Gaza will become its own thing \\n\\nGaza is pr...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  upper\n",
       "0  Is this just \"The Jew Show\" now, or is this ju...      1\n",
       "1  Wiehe points out that masturbation, hitherto a...      0\n",
       "2  Wiehe points out that masturbation, hitherto a...      0\n",
       "3  You think the forcible removal of people is un...      1\n",
       "4  >Gaza will become its own thing \\n\\nGaza is pr...      6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count occurences of uppercase words, which may indicate rage or anger\n",
    "\n",
    "df['upper'] = df['body'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "df[['body','upper']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEXT CLEANUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body'] = df['body'].apply(lambda x: x.lower());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note -- Important to run stop words before removing punctuation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              \"the jew show\" now, reality living nyc?\n",
       "1    wiehe points masturbation, hitherto hole-in-co...\n",
       "2    wiehe points masturbation, hitherto hole-in-co...\n",
       "3    think forcible removal people unacceptable? ma...\n",
       "4    >gaza become thing gaza predicted collapse ent...\n",
       "Name: body, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "df['body'] = df['body'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "df.body.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body'] = df['body'].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "omnisentient                                   1\n",
       "infeasible                                     1\n",
       "midtown                                        1\n",
       "septaugaint                                    1\n",
       "pagehttpsenwikipediaorgwikisweden_democrats    1\n",
       "schwager                                       1\n",
       "entrustersnear                                 1\n",
       "principlesim                                   1\n",
       "whiteswhile                                    1\n",
       "cfai                                           1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(df['body']).split()).value_counts()[-10:]\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10045833"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10M word/token corpus  \n",
    "\n",
    "df.word_count.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>permalink</th>\n",
       "      <th>s_author</th>\n",
       "      <th>s_domain</th>\n",
       "      <th>s_score</th>\n",
       "      <th>s_title</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>word_count</th>\n",
       "      <th>cowbell</th>\n",
       "      <th>upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>bat_mayn</td>\n",
       "      <td>the jew show now reality living nyc</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-06-26 19:57:00+00:00</td>\n",
       "      <td>30555910741</td>\n",
       "      <td>534217119</td>\n",
       "      <td>534217119</td>\n",
       "      <td>/r/TACN/comments/8u24kf/tacs_652_show_discussi...</td>\n",
       "      <td>jakdak</td>\n",
       "      <td>i.imgur.com</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TACS 652 | Show Discussion</td>\n",
       "      <td>1</td>\n",
       "      <td>TACN</td>\n",
       "      <td>1411.0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    author                                 body  \\\n",
       "0           0  bat_mayn  the jew show now reality living nyc   \n",
       "\n",
       "   controversiality                created_utc           id    link_id  \\\n",
       "0                 0  2018-06-26 19:57:00+00:00  30555910741  534217119   \n",
       "\n",
       "   parent_id                                          permalink s_author  \\\n",
       "0  534217119  /r/TACN/comments/8u24kf/tacs_652_show_discussi...   jakdak   \n",
       "\n",
       "      s_domain  s_score                     s_title  score subreddit  \\\n",
       "0  i.imgur.com      1.0  TACS 652 | Show Discussion      1      TACN   \n",
       "\n",
       "   subreddit_subscribers  word_count  cowbell  upper  \n",
       "0                 1411.0          17        0      1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SENTIMENT ANALYSIS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using raw text, without pre-processing first produces poor results for word2vec. Similiar words are lumped together with misspellings and variations of the original word. Recommend comprehensive pre-processing before building a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = np.array(df['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['the jew show now reality living nyc',\n",
       "       'wiehe points masturbation hitherto holeincorner vice began shamelessly promoted first time weimar germany jewishrun organizations httpswwwveteranstodaycom20130924sexualdecadenceweimargermany'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize words \n",
    "\n",
    "documents = [x.split(' ') for x in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'jew', 'show', 'now', 'reality', 'living', 'nyc']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(\n",
    "        documents,\n",
    "        size=150,\n",
    "        window=10,\n",
    "        min_count=10,\n",
    "        workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(documents, total_examples=len(df['body']), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 'holocaust'\n",
    "model.wv.most_similar (positive = w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 'zionist'\n",
    "model.wv.most_similar (positive = w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 'jew'\n",
    "model.wv.most_similar (positive = w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 'kike'\n",
    "model.wv.most_similar (positive = w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 'hitler'\n",
    "model.wv.most_similar (positive = w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 'soros'\n",
    "model.wv.most_similar (positive = w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEXTBLOB SENTIMENT ANALYZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_finder(comment):\n",
    "    \n",
    "    analysis = TextBlob(comment)\n",
    "    \n",
    "    if analysis.sentiment.polarity > 0: \n",
    "        return 1 \n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 0\n",
    "    else: \n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SA'] = np.array([ sentiment_finder(comment) for comment in df['body'] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>permalink</th>\n",
       "      <th>s_author</th>\n",
       "      <th>s_domain</th>\n",
       "      <th>s_score</th>\n",
       "      <th>s_title</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>word_count</th>\n",
       "      <th>cowbell</th>\n",
       "      <th>upper</th>\n",
       "      <th>SA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>bat_mayn</td>\n",
       "      <td>the jew show now reality living nyc</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-06-26 19:57:00+00:00</td>\n",
       "      <td>30555910741</td>\n",
       "      <td>534217119</td>\n",
       "      <td>534217119</td>\n",
       "      <td>/r/TACN/comments/8u24kf/tacs_652_show_discussi...</td>\n",
       "      <td>jakdak</td>\n",
       "      <td>i.imgur.com</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TACS 652 | Show Discussion</td>\n",
       "      <td>1</td>\n",
       "      <td>TACN</td>\n",
       "      <td>1411.0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    author                                 body  \\\n",
       "0           0  bat_mayn  the jew show now reality living nyc   \n",
       "\n",
       "   controversiality                created_utc           id    link_id  \\\n",
       "0                 0  2018-06-26 19:57:00+00:00  30555910741  534217119   \n",
       "\n",
       "   parent_id                                          permalink s_author  \\\n",
       "0  534217119  /r/TACN/comments/8u24kf/tacs_652_show_discussi...   jakdak   \n",
       "\n",
       "      s_domain  s_score                     s_title  score subreddit  \\\n",
       "0  i.imgur.com      1.0  TACS 652 | Show Discussion      1      TACN   \n",
       "\n",
       "   subreddit_subscribers  word_count  cowbell  upper  SA  \n",
       "0                 1411.0          17        0      1   0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "worldnews    2578\n",
       "politics     2137\n",
       "AskReddit    1454\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['SA'] < 0].subreddit.value_counts()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_finder_partial(comment):\n",
    "    analysis = TextBlob(comment)\n",
    "    return analysis.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SA_p'] = np.array([ sentiment_finder_partial(comment) for comment in df['body'] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>permalink</th>\n",
       "      <th>s_author</th>\n",
       "      <th>...</th>\n",
       "      <th>s_score</th>\n",
       "      <th>s_title</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>word_count</th>\n",
       "      <th>cowbell</th>\n",
       "      <th>upper</th>\n",
       "      <th>SA</th>\n",
       "      <th>SA_p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>bat_mayn</td>\n",
       "      <td>the jew show now reality living nyc</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-06-26 19:57:00+00:00</td>\n",
       "      <td>30555910741</td>\n",
       "      <td>534217119</td>\n",
       "      <td>534217119</td>\n",
       "      <td>/r/TACN/comments/8u24kf/tacs_652_show_discussi...</td>\n",
       "      <td>jakdak</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TACS 652 | Show Discussion</td>\n",
       "      <td>1</td>\n",
       "      <td>TACN</td>\n",
       "      <td>1411.0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    author                                 body  \\\n",
       "0           0  bat_mayn  the jew show now reality living nyc   \n",
       "\n",
       "   controversiality                created_utc           id    link_id  \\\n",
       "0                 0  2018-06-26 19:57:00+00:00  30555910741  534217119   \n",
       "\n",
       "   parent_id                                          permalink s_author  \\\n",
       "0  534217119  /r/TACN/comments/8u24kf/tacs_652_show_discussi...   jakdak   \n",
       "\n",
       "   ...  s_score                     s_title score  subreddit  \\\n",
       "0  ...      1.0  TACS 652 | Show Discussion     1       TACN   \n",
       "\n",
       "  subreddit_subscribers  word_count  cowbell  upper  SA  SA_p  \n",
       "0                1411.0          17        0      1   0   0.0  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndQwHSVLHcJAkdQwHSVLHcJAkdf4vU2SDP0MT4RgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a4b3ebe48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df.SA_p);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_comments = [ comment for index, comment in enumerate(df['body']) if df['SA'][index] > 0]\n",
    "# neu_comments = [ comment for index, comment in enumerate(df['body']) if df['SA'][index] == 0]\n",
    "# neg_comments = [ comment for index, comment in enumerate(df['body']) if df['SA'][index] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Percentage of positive comments: {:.1f}%\".format(len(pos_comments)*100.0/len(df['body'])))\n",
    "# print(\"Percentage of neutral comments: {:.1f}%\".format(len(neu_comments)*100.0/len(df['body'])))\n",
    "# print(\"Percentage de negative comments: {:.1f}%\".format(len(neg_comments)*100.0/len(df['body'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = df[df['SA'] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('author')['body'].count().sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df[df['author'] == 'send_nasty_stuff'].subreddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T_SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfv = pd.Series(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = pd.concat([dfv, df['subreddit']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VADER SENTIMENT ANALYZER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The food is good and the atmosphere is nice'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'The food is good and the atmosphere is nice'\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compound': 0.6908, 'neg': 0.0, 'neu': 0.551, 'pos': 0.449}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snt = analyser.polarity_scores(s) # polarity_scores(s)\n",
    "snt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.449"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snt['pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_finder_vader(comment):\n",
    "    #return the raw compound score \n",
    "    # if > .05, this is positve sentiment\n",
    "    # if < .05, this is negative sentiment \n",
    "    # otherwise neutral     \n",
    "        \n",
    "    analysis = analyser.polarity_scores(comment)\n",
    "    \n",
    "    return analysis['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Vader'] = np.array([ sentiment_finder_vader(comment) for comment in df['body'] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_vader = df.groupby('subreddit')['Vader'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>permalink</th>\n",
       "      <th>s_author</th>\n",
       "      <th>...</th>\n",
       "      <th>s_title</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>word_count</th>\n",
       "      <th>cowbell</th>\n",
       "      <th>upper</th>\n",
       "      <th>SA</th>\n",
       "      <th>SA_p</th>\n",
       "      <th>Vader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>bat_mayn</td>\n",
       "      <td>the jew show now reality living nyc</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-06-26 19:57:00+00:00</td>\n",
       "      <td>30555910741</td>\n",
       "      <td>534217119</td>\n",
       "      <td>534217119</td>\n",
       "      <td>/r/TACN/comments/8u24kf/tacs_652_show_discussi...</td>\n",
       "      <td>jakdak</td>\n",
       "      <td>...</td>\n",
       "      <td>TACS 652 | Show Discussion</td>\n",
       "      <td>1</td>\n",
       "      <td>TACN</td>\n",
       "      <td>1411.0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    author                                 body  \\\n",
       "0           0  bat_mayn  the jew show now reality living nyc   \n",
       "\n",
       "   controversiality                created_utc           id    link_id  \\\n",
       "0                 0  2018-06-26 19:57:00+00:00  30555910741  534217119   \n",
       "\n",
       "   parent_id                                          permalink s_author  \\\n",
       "0  534217119  /r/TACN/comments/8u24kf/tacs_652_show_discussi...   jakdak   \n",
       "\n",
       "   ...                       s_title  score subreddit  subreddit_subscribers  \\\n",
       "0  ...    TACS 652 | Show Discussion      1      TACN                 1411.0   \n",
       "\n",
       "  word_count  cowbell  upper  SA  SA_p  Vader  \n",
       "0         17        0      1   0   0.0    0.0  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"clean_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"clean_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = df.groupby(['subreddit'])['subreddit_subscribers'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = df.groupby(['subreddit'])['body'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti = df.groupby('subreddit')['SA'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = sns.distplot(senti_vader, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1 = sns.scatterplot(subs, comments, hue=senti, );\n",
    "\n",
    "ax1.set_ylabel('Comment Count');\n",
    "ax1.set_xlabel('Subreddit Sub Count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2 = sns.scatterplot(subs, comments, hue=senti_vader, );\n",
    "\n",
    "ax2.set_ylabel('Comment Count');\n",
    "ax2.set_xlabel('Subreddit Sub Count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = df.groupby(['subreddit'])['word_count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style.available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_score = df.groupby(['subreddit'])['s_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "style.use('seaborn-white')\n",
    "\n",
    "cmap = sns.diverging_palette(240, 10, n=20, as_cmap=True)\n",
    "ax3 = sns.scatterplot(word_count, comments, hue=senti_vader, palette=cmap\n",
    "                      );\n",
    "\n",
    "ax3.set_ylabel('Comment Count');\n",
    "ax3.set_xlabel('Word Count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Vader'] == df['Vader'].min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(df1[df1['author'] == 'LivingWater404'][3:]['body'])[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOPIC MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NMF - COUNT VECTORIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction.text as text\n",
    "\n",
    "vectorizer = text.CountVectorizer(min_df=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 40000\n",
    "n_features = 2000\n",
    "n_topics = 5\n",
    "n_top_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = vectorizer.fit_transform(df['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115420, 33847)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33847"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = decomposition.NMF(n_components=n_topics, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctopic = clf.fit_transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words = []\n",
    "\n",
    "for topic in clf.components_:\n",
    "    word_idx = np.argsort(topic)[::-1][0:n_top_words]\n",
    "    topic_words.append([vocab[i] for i in word_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['would', 'one', 'like', 'even', 'also', 'think', 'way', 'time', 'us', 'many']\n",
      "['im', 'it', 'bee', 'know', 'thats', 'right', 'bees', 'you', 'like', 'get']\n",
      "['people', 'said', 'kike', 'white', 'black', 'like', 'think', 'want', 'racist', 'jewish']\n",
      "['god', 'jesus', 'christ', 'one', 'believe', 'life', 'lord', 'bible', 'man', 'gods']\n",
      "['jews', 'israel', 'jewish', 'palestinians', 'land', 'state', 'israeli', 'palestine', 'arab', 'arabs']\n"
     ]
    }
   ],
   "source": [
    "for topic in topic_words:\n",
    "    print(topic[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicmax = list()\n",
    "for topic in doctopic:\n",
    "    topicmax.append(argmax(topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicmax = np.array(topicmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['topicmax'] = topicmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    52879\n",
       "0    28820\n",
       "2    16382\n",
       "1    12448\n",
       "3     4891\n",
       "Name: topicmax, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.topicmax.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "politics                1034\n",
       "worldnews                956\n",
       "AskReddit                738\n",
       "milliondollarextreme     506\n",
       "conspiracy               439\n",
       "news                     427\n",
       "CringeAnarchy            396\n",
       "The_Donald               380\n",
       "pics                     254\n",
       "Judaism                  226\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.topicmax == 2].subreddit.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('clean_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NMF - TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df = 5, max_df = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = tfidf_vectorizer.fit_transform(df['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vocab = np.array(tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = decomposition.NMF(n_components=n_topics, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_doctopic = clf.fit_transform(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words = []\n",
    "\n",
    "for topic in clf.components_:\n",
    "    word_idx = np.argsort(topic)[::-1][0:n_top_words]\n",
    "    topic_words.append([tfidf_vocab[i] for i in word_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in topic_words:\n",
    "    print(topic[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rerun after lemmatizing corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatiz(comment):\n",
    "    return [lemmatizer.lemmatize(w) for w in comment.split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df['body'][5]\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatiz(t);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemma'] = np.array([ lemmatiz(comment) for comment in df['body'] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemma'] = df['lemma'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = tfidf_vectorizer.fit_transform(df['lemma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vocab = np.array(tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = decomposition.NMF(n_components=6, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_doctopic = clf.fit_transform(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words = []\n",
    "\n",
    "for topic in clf.components_:\n",
    "    word_idx = np.argsort(topic)[::-1][0:n_top_words]\n",
    "    topic_words.append([tfidf_vocab[i] for i in word_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in topic_words:\n",
    "    print(topic[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LEMMATIZE - NMF - COUNT VECTORIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = vectorizer.fit_transform(df['lemma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = decomposition.NMF(n_components=6, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctopic = clf.fit_transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words = []\n",
    "\n",
    "for topic in clf.components_:\n",
    "    word_idx = np.argsort(topic)[::-1][0:n_top_words]\n",
    "    topic_words.append([vocab[i] for i in word_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in topic_words:\n",
    "    print(topic[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-MEANS CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 5\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "km.fit(tfidf_doctopic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = np.array(km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# dist = 1 - cosine_similarity(tfidf_doctopic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments = { 'subreddit': df['subreddit'], 'author': df['author'], 'cluster': clusters, }\n",
    "\n",
    "# frame = pd.DataFrame(comments, index = [clusters] , columns = ['rank', 'title', 'cluster', 'genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scattertext - compare subreddit word frequencies. \n",
    "\n",
    "https://github.com/JasonKessler/scattertext\n",
    "\n",
    "- the_donald vs alt.right \n",
    "- politics vs worldnews \n",
    "- worldnews vs worldpolitics \n",
    "- israel vs palestine(?)/arab subreddits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
